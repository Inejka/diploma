{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import gradio as gr\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import sqlite3\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "class CollectionsDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 csv_file,\n",
    "                 root_dir,\n",
    "                 num_classes,\n",
    "                 transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.data.loc[idx, 'img_name'])\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        label_tensor = torch.zeros(self.num_classes)\n",
    "        label_tensor[self.data.loc[idx, 'score']-1] = 1\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return {'image': image,\n",
    "                'labels': label_tensor\n",
    "                }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import pretrainedmodels as pm\n",
    "\n",
    "model = pm.__dict__[\"resnet101\"](pretrained='imagenet')\n",
    "\n",
    "model.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "model.last_linear = nn.Sequential(\n",
    "    nn.BatchNorm1d(2048),\n",
    "    nn.Dropout(p=0.25),\n",
    "    nn.Linear(in_features=2048, out_features=2048),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(2048, eps=1e-05, momentum=0.1),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(in_features=2048, out_features=10),\n",
    ")\n",
    "# model.load_state_dict(torch.load(\"model.bin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,\n",
    "                data_loader,\n",
    "                dataset_size,\n",
    "                optimizer,\n",
    "                scheduler,\n",
    "                num_epochs):\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        scheduler.step()\n",
    "        model.train()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        # Iterate over data.\n",
    "        for bi, d in enumerate(data_loader):\n",
    "            inputs = d[\"image\"]\n",
    "            labels = d[\"labels\"]\n",
    "            inputs = inputs.to(device, dtype=torch.float)\n",
    "            labels = labels.to(device, dtype=torch.float)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        epoch_loss = running_loss / dataset_size\n",
    "        print('Loss: {:.4f}'.format(epoch_loss))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "# define some re-usable stuff\n",
    "IMAGE_SIZE = 512\n",
    "BATCH_SIZE = 9\n",
    "device = torch.device(\"cuda:0\")\n",
    "IMG_MEAN = model.mean\n",
    "IMG_STD = model.std\n",
    "\n",
    "# make some augmentations on training data\n",
    "train_transform=transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    transforms.CenterCrop(IMAGE_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMG_MEAN, IMG_STD)\n",
    "])\n",
    "\n",
    "\n",
    "# use the collections dataset class we created earlier\n",
    "CSV_FILE_TRAIN = r\"dataset\\balanced\\data.csv\"\n",
    "ROOT_DIR_TRAIN = r\"dataset\\balanced\"\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "train_dataset = CollectionsDataset(CSV_FILE_TRAIN,ROOT_DIR_TRAIN,NUM_CLASSES, train_transform)\n",
    "\n",
    "# create the pytorch data loader\n",
    "train_dataset_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                   batch_size=BATCH_SIZE,\n",
    "                                                   shuffle=True)\n",
    "# push model to device\n",
    "model_ft = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "plist = [\n",
    "        {'params': model_ft.layer4.parameters(), 'lr': 1e-5},\n",
    "        {'params': model_ft.last_linear.parameters(), 'lr': 5e-3}\n",
    "        ]\n",
    "optimizer_ft = optim.Adam(plist, lr=0.001)\n",
    "lr_sch = lr_scheduler.StepLR(optimizer_ft, step_size=10, gamma=0.1)\n",
    "\n",
    "\n",
    "# model_ft = train_model(model_ft,\n",
    "#                        train_dataset_loader,\n",
    "#                        len(train_dataset),\n",
    "#                        optimizer_ft,\n",
    "#                        lr_sch,\n",
    "#                        num_epochs=5)\n",
    "#\n",
    "# torch.save(model_ft.state_dict(), \"model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_BATCH_SIZE = 10\n",
    "test_transform=transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    transforms.CenterCrop(IMAGE_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMG_MEAN,IMG_STD)\n",
    "])\n",
    "\n",
    "model_ft.load_state_dict(torch.load(\"model.bin\"))\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "for param in model_ft.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_ft.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reznya = Image.open(r\"random_photos\\image.jpg\").convert('RGB')\n",
    "reznya = train_transform(reznya).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.argmax(model_ft(reznya.unsqueeze(0)).sigmoid()).item()+1"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
